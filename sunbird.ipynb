{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde6a75",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from compilation import Assembly, C, Compiler\n",
    "from compression_filter import CompressionFilter\n",
    "from datapoint import DataPoint, Label\n",
    "from dataset_iterator import DatasetIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df1650-fc8c-459a-be4a-4106a5be1092",
   "metadata": {},
   "source": [
    "# model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3583024-818d-4c83-8d96-8e4637aa9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sunbird(nn.Module):\n",
    "    def __init__(  # noqa: PLR0913\n",
    "        self,\n",
    "        *,\n",
    "        src_type_vocab_size: int,\n",
    "        src_text_vocab_size: int,\n",
    "        trg_type_vocab_size: int,\n",
    "        trg_text_vocab_size: int,\n",
    "        src_pad_idx: int,\n",
    "        max_datapoint_len: int,\n",
    "        d_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 6,\n",
    "        d_ff: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_type_embedding = nn.Embedding(src_type_vocab_size, d_model)\n",
    "        self.input_text_embedding = nn.Embedding(src_text_vocab_size, d_model)\n",
    "        self.input_position_embedding = nn.Embedding(max_datapoint_len, d_model)\n",
    "\n",
    "        self.output_type_embedding = nn.Embedding(trg_type_vocab_size, d_model)\n",
    "        self.output_text_embedding = nn.Embedding(trg_text_vocab_size, d_model)\n",
    "        self.output_position_embedding = nn.Embedding(max_datapoint_len, d_model)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            num_layers,\n",
    "            d_ff,\n",
    "            dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc_in_src = nn.Linear(2 * d_model, d_model)\n",
    "        self.fc_in_trg = nn.Linear(2 * d_model, d_model)\n",
    "\n",
    "        self.fc_type_out = nn.Linear(d_model, trg_type_vocab_size)\n",
    "        self.fc_text_out = nn.Linear(d_model, trg_text_vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src_type_ids: Tensor,\n",
    "        src_text_ids: Tensor,\n",
    "        trg_type_ids: Tensor,\n",
    "        trg_text_ids: Tensor,\n",
    "    ) -> tuple[Tensor, Tensor]:\n",
    "        device = src_type_ids.device\n",
    "        batch_size, max_seq_len = src_type_ids.shape\n",
    "\n",
    "        src_type_embedded = self.dropout(self.input_type_embedding(src_type_ids))\n",
    "        src_text_embedded = self.dropout(self.input_text_embedding(src_text_ids))\n",
    "        src_positions = self.input_position_embedding(\n",
    "            torch.arange(max_seq_len, device=device),\n",
    "        )\n",
    "        src_embedded = self.dropout(\n",
    "            self.fc_in_src(torch.cat((src_type_embedded, src_text_embedded), dim=-1))\n",
    "            + src_positions,\n",
    "        )\n",
    "\n",
    "        trg_type_embedded = self.dropout(self.output_type_embedding(trg_type_ids))\n",
    "        trg_text_embedded = self.dropout(self.output_text_embedding(trg_text_ids))\n",
    "        trg_positions = self.output_position_embedding(\n",
    "            torch.arange(max_seq_len, device=device),\n",
    "        )\n",
    "        trg_embedded = self.dropout(\n",
    "            self.fc_in_trg(torch.cat((trg_type_embedded, trg_text_embedded), dim=-1))\n",
    "            + trg_positions,\n",
    "        )\n",
    "\n",
    "        src_mask = src_type_ids.transpose(0, 1) == self.src_pad_idx\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_seq_len)\n",
    "\n",
    "        transformer_output = self.transformer(\n",
    "            src_embedded,\n",
    "            trg_embedded,\n",
    "            src_key_padding_mask=src_mask.transpose(0, 1),\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "        return (\n",
    "            self.fc_type_out(transformer_output),\n",
    "            self.fc_text_out(transformer_output),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13676e1-9867-4c25-ba2c-c6d76270cb52",
   "metadata": {},
   "source": [
    "# generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529fbfd-1769-499d-b001-f2607ca8b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(  # noqa: PLR0913\n",
    "    model: Sunbird,\n",
    "    asm_code: str,\n",
    "    vocab: Vocab,\n",
    "    max_generation_length: int,\n",
    "    max_inp_len: int,\n",
    "    device: torch.device,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    dp = DataPoint(Label(0), C(\"\"))\n",
    "    dp.asm.append(Assembly(asm_code, Compiler(), 0))\n",
    "    token_pairs = CompressionFilter(dp).all_tokens()[0][0]\n",
    "\n",
    "    types: list[int] = []\n",
    "    texts: list[int] = []\n",
    "\n",
    "    for typ, txt in token_pairs:\n",
    "        types.append(vocab.src_type[typ])\n",
    "        texts.append(vocab.src_text[txt])\n",
    "\n",
    "    def pad_or_truncate(vec: list[int]) -> list[int]:\n",
    "        if len(vec) < max_inp_len:\n",
    "            vec = vec + [vocab.src_pad_idx] * (max_inp_len - len(vec))\n",
    "        elif len(vec) > max_inp_len:\n",
    "            vec = vec[:max_inp_len]\n",
    "        return vec\n",
    "\n",
    "    type_tensor: Tensor = torch.tensor(pad_or_truncate(types), device=device).unsqueeze(\n",
    "        0,\n",
    "    )\n",
    "    text_tensor: Tensor = torch.tensor(pad_or_truncate(texts), device=device).unsqueeze(\n",
    "        0,\n",
    "    )\n",
    "\n",
    "    generated_types = torch.tensor(\n",
    "        pad_or_truncate([vocab.bos_idx]),\n",
    "        device=device,\n",
    "    ).unsqueeze(0)\n",
    "    generated_texts = torch.tensor(\n",
    "        pad_or_truncate([vocab.bos_idx]),\n",
    "        device=device,\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    type_gen_idx = 1\n",
    "    text_gen_idx = 1\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_generation_length):\n",
    "            o_type, o_text = model(\n",
    "                type_tensor,\n",
    "                text_tensor,\n",
    "                generated_types,\n",
    "                generated_texts,\n",
    "            )\n",
    "\n",
    "            next_type_logits = o_type[:, -1, :]\n",
    "            next_text_logits = o_text[:, -1, :]\n",
    "\n",
    "            next_type_probs = nn.functional.softmax(next_type_logits, dim=-1)\n",
    "            next_text_probs = nn.functional.softmax(next_text_logits, dim=-1)\n",
    "\n",
    "            next_type = torch.multinomial(next_type_probs, num_samples=1)\n",
    "            next_text = torch.multinomial(next_text_probs, num_samples=1)\n",
    "\n",
    "            generated_types[0][type_gen_idx] = next_type.item()\n",
    "            generated_texts[0][text_gen_idx] = next_text.item()\n",
    "\n",
    "            type_gen_idx += 1\n",
    "            text_gen_idx += 1\n",
    "\n",
    "            if next_type.item() == vocab.eos_idx or next_text.item() == vocab.eos_idx:\n",
    "                break\n",
    "\n",
    "    inv_type = {v: k for k, v in vocab.trg_type.items()}\n",
    "    inv_text = {v: k for k, v in vocab.trg_text.items()}\n",
    "\n",
    "    inv_type[0] = \"<pad>\"\n",
    "    inv_text[0] = \"<pad>\"\n",
    "\n",
    "    inv_type[1] = \"<bos>\"\n",
    "    inv_text[1] = \"<bos>\"\n",
    "\n",
    "    inv_type[2] = \"<eos>\"\n",
    "    inv_text[2] = \"<eos>\"\n",
    "\n",
    "    str_types = [inv_type.get(x, \"<unk>\") for x in generated_types[0].tolist()]\n",
    "    str_texts = [inv_text.get(x, \"<unk>\") for x in generated_texts[0].tolist()]\n",
    "\n",
    "    return (\n",
    "        [x for x in str_types if x != \"<pad>\"],\n",
    "        [x for x in str_texts if x != \"<pad>\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea951d18-a806-4a0b-95b6-72a47b88807c",
   "metadata": {},
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4f07f-b810-45de-a56e-8ff20a5c8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self) -> None:\n",
    "        self.src_pad_idx = 0\n",
    "        self.bos_idx = 1\n",
    "        self.eos_idx = 2\n",
    "        # reserving [0] for pad\n",
    "        self.src_type: dict[str, int] = defaultdict(\n",
    "            lambda: len(self.src_type) + 1,\n",
    "        )\n",
    "        self.src_text: dict[str, int] = defaultdict(\n",
    "            lambda: len(self.src_text) + 1,\n",
    "        )\n",
    "        # reserving [1] for bos and [2] for eos\n",
    "        self.trg_type: dict[str, int] = defaultdict(\n",
    "            lambda: len(self.trg_type) + 3,\n",
    "        )\n",
    "        self.trg_text: dict[str, int] = defaultdict(\n",
    "            lambda: len(self.trg_text) + 3,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645746da-9527-43fe-98a8-c170273bf310",
   "metadata": {},
   "source": [
    "# batch processing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c915a68-73cf-457b-8a02-395274e01e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchBuilder:\n",
    "    def __init__(  # noqa: PLR0913\n",
    "        self,\n",
    "        dataset_csv_path: str,\n",
    "        batch_size: int,\n",
    "        max_seq_len: int,\n",
    "        vocab: Vocab,\n",
    "        device: torch.device,\n",
    "        compile_chunk_size: int = 1024,\n",
    "    ) -> None:\n",
    "        self.iterator = DatasetIterator(dataset_csv_path)\n",
    "        self.num_asm_per_c = len(\n",
    "            CompressionFilter(self.iterator.take(1)[0]).all_tokens(),\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        self.compile_chunk_size = compile_chunk_size\n",
    "\n",
    "        self.compiled: list[tuple[list[tuple[str, str]], list[tuple[str, str]]]] = []\n",
    "\n",
    "    def __pad_or_truncate(self, vec: list[int]) -> list[int]:\n",
    "        if len(vec) < self.max_seq_len:\n",
    "            vec = vec + [self.vocab.src_pad_idx] * (max_seq_len - len(vec))\n",
    "        elif len(vec) > self.max_seq_len:\n",
    "            vec = vec[: self.max_seq_len]\n",
    "        return vec\n",
    "\n",
    "    def next_batch(self) -> tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        if len(self.compiled) < self.batch_size:\n",
    "            self.compiled.extend(\n",
    "                [\n",
    "                    pair\n",
    "                    for dp in self.iterator.take(self.compile_chunk_size)\n",
    "                    for pair in CompressionFilter(dp).all_tokens()\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        src_types, src_texts, trg_types, trg_texts = [], [], [], []\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            asm_pairs, c_pairs = self.compiled.pop(0)\n",
    "\n",
    "            src_types.append(\n",
    "                self.__pad_or_truncate(\n",
    "                    [self.vocab.src_type[pair[0]] for pair in asm_pairs],\n",
    "                ),\n",
    "            )\n",
    "            src_texts.append(\n",
    "                self.__pad_or_truncate(\n",
    "                    [self.vocab.src_text[pair[1]] for pair in asm_pairs],\n",
    "                ),\n",
    "            )\n",
    "            trg_types.append(\n",
    "                self.__pad_or_truncate(\n",
    "                    [self.vocab.bos_idx]\n",
    "                    + [self.vocab.trg_type[pair[0]] for pair in c_pairs]\n",
    "                    + [self.vocab.eos_idx],\n",
    "                ),\n",
    "            )\n",
    "            trg_texts.append(\n",
    "                self.__pad_or_truncate(\n",
    "                    [self.vocab.bos_idx]\n",
    "                    + [self.vocab.trg_text[pair[1]] for pair in c_pairs]\n",
    "                    + [self.vocab.eos_idx],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            torch.tensor(src_types, device=self.device),\n",
    "            torch.tensor(src_texts, device=self.device),\n",
    "            torch.tensor(trg_types, device=self.device),\n",
    "            torch.tensor(trg_texts, device=self.device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bb258-3b89-46dd-9f38-65ee2a366a47",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a1b9f-fb08-435a-9cd2-8c15351988ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(  # noqa: PLR0913\n",
    "    *,\n",
    "    model: nn.Module,\n",
    "    num_epochs: int,\n",
    "    src_pad_idx: int,\n",
    "    trg_type_vocab_size: int,\n",
    "    trg_text_vocab_size: int,\n",
    "    batch_builder: BatchBuilder,\n",
    "    learning_rate: float = 3e-4,\n",
    "    save_interval: int = 5,\n",
    "    num_batches_per_epoch: int = 100,\n",
    "    save_dir: str = \"checkpoints\",\n",
    ") -> None:\n",
    "    # import error fixed in pytorch upstream fb1c580\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # type: ignore[attr-defined]\n",
    "    criterion_type = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "    criterion_text = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "\n",
    "    Path(save_dir).mkdir(exist_ok=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            range(num_batches_per_epoch),\n",
    "            desc=f\"epoch {epoch+1}/{num_epochs}\",\n",
    "        )\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            src_type_ids, src_text_ids, trg_type_ids, trg_text_ids = (\n",
    "                batch_builder.next_batch()\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            type_output, text_output = model(\n",
    "                src_type_ids,\n",
    "                src_text_ids,\n",
    "                trg_type_ids,\n",
    "                trg_text_ids,\n",
    "            )\n",
    "\n",
    "            loss_type = criterion_type(\n",
    "                type_output.view(-1, trg_type_vocab_size),\n",
    "                trg_type_ids.view(-1),\n",
    "            )\n",
    "            loss_text = criterion_text(\n",
    "                text_output.view(-1, trg_text_vocab_size),\n",
    "                trg_text_ids.view(-1),\n",
    "            )\n",
    "            loss = loss_type + loss_text\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": total_loss / (batch + 1)})\n",
    "\n",
    "        avg_loss = total_loss / num_batches_per_epoch\n",
    "        print(f\"epoch {epoch+1}/{num_epochs}, average loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            checkpoint_path = Path(save_dir) / f\"sunbird_model_epoch_{epoch+1}.pt\"\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"loss\": avg_loss,\n",
    "                },\n",
    "                checkpoint_path,\n",
    "            )\n",
    "            print(f\"model saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0dd7b-4d5a-44f9-84fd-6e4861b2d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 1\n",
    "max_seq_len: int = 2048  # ~= 95th percentile for tokenized asm code length\n",
    "\n",
    "# calculated based off train.csv\n",
    "src_type_vocab_size = 22\n",
    "src_text_vocab_size = 110000\n",
    "trg_type_vocab_size = 112\n",
    "trg_text_vocab_size = 40000\n",
    "\n",
    "epochs: int = 100\n",
    "batches_per_epoch: int = 100\n",
    "save_interval = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d42285-8275-4eda-becf-e210309923e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training on {torch.cuda.get_device_name(torch.cuda.current_device())}...\")\n",
    "\n",
    "vocab = Vocab()\n",
    "\n",
    "batch_builder = BatchBuilder(\n",
    "    \"dataset/train.csv\",\n",
    "    batch_size,\n",
    "    max_seq_len,\n",
    "    vocab,\n",
    "    device,\n",
    ")\n",
    "\n",
    "model = Sunbird(\n",
    "    src_type_vocab_size=src_type_vocab_size,\n",
    "    src_text_vocab_size=src_text_vocab_size,\n",
    "    trg_type_vocab_size=trg_type_vocab_size,\n",
    "    trg_text_vocab_size=trg_text_vocab_size,\n",
    "    src_pad_idx=vocab.src_pad_idx,\n",
    "    max_datapoint_len=max_seq_len,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "training_loop(\n",
    "    model=model,\n",
    "    num_epochs=epochs,\n",
    "    num_batches_per_epoch=batches_per_epoch,\n",
    "    src_pad_idx=vocab.src_pad_idx,\n",
    "    trg_type_vocab_size=trg_type_vocab_size,\n",
    "    trg_text_vocab_size=trg_text_vocab_size,\n",
    "    batch_builder=batch_builder,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
