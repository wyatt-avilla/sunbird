{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde6a75",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING, Generator\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from data import DataPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df1650-fc8c-459a-be4a-4106a5be1092",
   "metadata": {},
   "source": [
    "# model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3583024-818d-4c83-8d96-8e4637aa9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sunbird(nn.Module):\n",
    "    def __init__(  # noqa: PLR0913\n",
    "        self,\n",
    "        *,\n",
    "        src_type_vocab_size: int,\n",
    "        src_text_vocab_size: int,\n",
    "        trg_type_vocab_size: int,\n",
    "        trg_text_vocab_size: int,\n",
    "        src_pad_idx: int,\n",
    "        max_datapoint_len: int,\n",
    "        d_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 6,\n",
    "        d_ff: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_type_embedding = nn.Embedding(src_type_vocab_size, d_model)\n",
    "        self.input_text_embedding = nn.Embedding(src_text_vocab_size, d_model)\n",
    "        self.input_position_embedding = nn.Embedding(max_datapoint_len, d_model)\n",
    "\n",
    "        self.output_type_embedding = nn.Embedding(trg_type_vocab_size, d_model)\n",
    "        self.output_text_embedding = nn.Embedding(trg_text_vocab_size, d_model)\n",
    "        self.output_position_embedding = nn.Embedding(max_datapoint_len, d_model)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            num_layers,\n",
    "            d_ff,\n",
    "            dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc_in_src = nn.Linear(2 * d_model, d_model)\n",
    "        self.fc_in_trg = nn.Linear(2 * d_model, d_model)\n",
    "\n",
    "        self.fc_type_out = nn.Linear(d_model, trg_type_vocab_size)\n",
    "        self.fc_text_out = nn.Linear(d_model, trg_text_vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src_type_ids: Tensor,\n",
    "        src_text_ids: Tensor,\n",
    "        trg_type_ids: Tensor,\n",
    "        trg_text_ids: Tensor,\n",
    "    ) -> tuple[Tensor, Tensor]:\n",
    "        device = src_type_ids.device\n",
    "        batch_size, max_seq_len = src_type_ids.shape\n",
    "\n",
    "        src_type_embedded = self.dropout(self.input_type_embedding(src_type_ids))\n",
    "        src_text_embedded = self.dropout(self.input_text_embedding(src_text_ids))\n",
    "        src_positions = self.input_position_embedding(\n",
    "            torch.arange(max_seq_len, device=device),\n",
    "        )\n",
    "        src_embedded = self.dropout(\n",
    "            self.fc_in_src(torch.cat((src_type_embedded, src_text_embedded), dim=-1))\n",
    "            + src_positions,\n",
    "        )\n",
    "\n",
    "        trg_type_embedded = self.dropout(self.output_type_embedding(trg_type_ids))\n",
    "        trg_text_embedded = self.dropout(self.output_text_embedding(trg_text_ids))\n",
    "        trg_positions = self.output_position_embedding(\n",
    "            torch.arange(max_seq_len, device=device),\n",
    "        )\n",
    "        trg_embedded = self.dropout(\n",
    "            self.fc_in_trg(torch.cat((trg_type_embedded, trg_text_embedded), dim=-1))\n",
    "            + trg_positions,\n",
    "        )\n",
    "\n",
    "        src_mask = src_type_ids.transpose(0, 1) == self.src_pad_idx\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_seq_len)\n",
    "\n",
    "        transformer_output = self.transformer(\n",
    "            src_embedded,\n",
    "            trg_embedded,\n",
    "            src_key_padding_mask=src_mask.transpose(0, 1),\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "        return (\n",
    "            self.fc_type_out(transformer_output),\n",
    "            self.fc_text_out(transformer_output),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab6083-2f13-4cbe-9ce1-867493f99dfc",
   "metadata": {},
   "source": [
    "# dataset interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22434800-727f-4e25-9641-1977e3094102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetInterface:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dataset: list[DataPoint],\n",
    "        max_inp_len: int,\n",
    "        pad_idx: int,\n",
    "    ) -> None:\n",
    "        self.dataset: list[DataPoint] = dataset\n",
    "        self.max_inp_len: int = max_inp_len\n",
    "        self.pad_idx: int = pad_idx\n",
    "        self.asm_type_map, self.asm_text_map, self.c_type_map, self.c_text_map = (\n",
    "            self.build_vocabulary(dataset)\n",
    "        )\n",
    "\n",
    "        self.generator = self.filtered_dataset()\n",
    "\n",
    "    def build_vocabulary(\n",
    "        self,\n",
    "        dataset: list[DataPoint],\n",
    "    ) -> tuple[dict[str, int], dict[str, int], dict[str, int], dict[str, int]]:\n",
    "        self.dataset = dataset\n",
    "        asm_type_map: dict[str, int] = defaultdict(lambda: len(asm_type_map) + 1)\n",
    "        asm_text_map: dict[str, int] = defaultdict(lambda: len(asm_text_map) + 1)\n",
    "\n",
    "        c_type_map: dict[str, int] = defaultdict(lambda: len(c_type_map) + 1)\n",
    "        c_text_map: dict[str, int] = defaultdict(lambda: len(c_text_map) + 1)\n",
    "\n",
    "        for dp in dataset:\n",
    "            for typ, txt in dp.c_code.as_tokens():\n",
    "                c_type_map[typ]\n",
    "                c_text_map[txt]\n",
    "            for asm_code in dp.asm:\n",
    "                for typ, txt in asm_code.as_tokens():\n",
    "                    asm_type_map[typ]\n",
    "                    asm_text_map[txt]\n",
    "\n",
    "        return (asm_type_map, asm_text_map, c_type_map, c_text_map)\n",
    "\n",
    "    def filtered_dataset(\n",
    "        self,\n",
    "    ) -> Generator[tuple[list[int], list[int], list[int], list[int]], None, None]:\n",
    "        for dp in self.dataset:\n",
    "            for asm in dp.asm:\n",
    "                asm_tokens = list(asm.as_tokens())\n",
    "                c_tokens = list(dp.c_code.as_tokens())\n",
    "                if len(asm_tokens) < self.max_inp_len:\n",
    "                    asm_types, asm_text = zip(*asm_tokens)\n",
    "                    c_types, c_text = zip(*c_tokens)\n",
    "                    yield (\n",
    "                        [self.asm_type_map[typ] for typ in asm_types]\n",
    "                        + [self.pad_idx] * (self.max_inp_len - len(asm_types)),\n",
    "                        [self.asm_text_map[txt] for txt in asm_text]\n",
    "                        + [self.pad_idx] * (self.max_inp_len - len(asm_text)),\n",
    "                        [self.c_type_map[typ] for typ in c_types]\n",
    "                        + [self.pad_idx] * (self.max_inp_len - len(c_types)),\n",
    "                        [self.c_text_map[txt] for txt in c_text]\n",
    "                        + [self.pad_idx] * (self.max_inp_len - len(c_text)),\n",
    "                    )\n",
    "\n",
    "    def take(self, n: int) -> Tensor:\n",
    "        return torch.tensor(list(zip(*[next(self.generator) for _ in range(n)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7abf1-0d9d-4e22-abb5-d48ca88d876d",
   "metadata": {},
   "source": [
    "# load generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pickle(input_file: str) -> list[DataPoint]:\n",
    "    with Path(input_file).open(\"rb\") as file:\n",
    "        return list(pickle.load(file))\n",
    "\n",
    "\n",
    "train: list[DataPoint] = from_pickle(\"dataset/train_compiled.pkl\")\n",
    "test: list[DataPoint] = from_pickle(\"dataset/test_compiled.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bb258-3b89-46dd-9f38-65ee2a366a47",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a1b9f-fb08-435a-9cd2-8c15351988ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(  # noqa: PLR0913\n",
    "    *,\n",
    "    model: nn.Module,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    src_pad_idx: int,\n",
    "    interface: DatasetInterface,\n",
    "    learning_rate: float = 3e-4,\n",
    "    save_interval: int = 5,\n",
    "    num_batches_per_epoch: int = 100,\n",
    "    save_dir: str = \"checkpoints\",\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # type: ignore[attr-defined]\n",
    "    # import error fixed in pytorch upstream fb1c580\n",
    "    criterion_type = nn.CrossEntropyLoss(\n",
    "        ignore_index=src_pad_idx,\n",
    "    )\n",
    "    criterion_text = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "\n",
    "    Path(save_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            range(num_batches_per_epoch),\n",
    "            desc=f\"epoch {epoch+1}/{num_epochs}\",\n",
    "        )\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            src_type_ids, src_text_ids, trg_type_ids, trg_text_ids = interface.take(\n",
    "                batch_size,\n",
    "            ).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            type_output, text_output = model(\n",
    "                src_type_ids,\n",
    "                src_text_ids,\n",
    "                trg_type_ids,\n",
    "                trg_text_ids,\n",
    "            )\n",
    "\n",
    "            loss_type = criterion_type(\n",
    "                type_output.view(-1, trg_type_vocab_size),\n",
    "                trg_type_ids.view(-1),\n",
    "            )\n",
    "            loss_text = criterion_text(\n",
    "                text_output.view(-1, trg_text_vocab_size),\n",
    "                trg_text_ids.view(-1),\n",
    "            )\n",
    "            loss = loss_type + loss_text\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": total_loss / (batch + 1)})\n",
    "\n",
    "        avg_loss = total_loss / num_batches_per_epoch\n",
    "        print(f\"epoch {epoch+1}/{num_epochs}, average loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            checkpoint_path = Path(save_dir) / f\"sunbird_model_epoch_{epoch+1}.pt\"\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"loss\": avg_loss,\n",
    "                },\n",
    "                checkpoint_path,\n",
    "            )\n",
    "            print(f\"model saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268aaf37-84e9-48c0-acf1-1ad6524d9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_c_snippets: int = len(train)  # 300m tokens\n",
    "num_asm_snippets: int = sum(len(dp.asm) for dp in train)  # 2.5b tokens\n",
    "\n",
    "print(\n",
    "    f\"training with {num_c_snippets} C snippets compiled to {num_asm_snippets} assembly snippets\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0dd7b-4d5a-44f9-84fd-6e4861b2d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 1\n",
    "max_seq_len: int = 2048  # ~= 95th percentile for tokenized asm code length\n",
    "src_pad_idx: int = 0\n",
    "\n",
    "interface = DatasetInterface(\n",
    "    dataset=train,\n",
    "    max_inp_len=max_seq_len,\n",
    "    pad_idx=src_pad_idx,\n",
    ")\n",
    "\n",
    "src_type_vocab_size: int = len(interface.asm_type_map)\n",
    "src_text_vocab_size: int = len(interface.asm_text_map)\n",
    "trg_type_vocab_size: int = len(interface.c_type_map)\n",
    "trg_text_vocab_size: int = len(interface.c_text_map)\n",
    "epochs: int = 50\n",
    "batches_per_epoch: int = 100\n",
    "save_interval = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d42285-8275-4eda-becf-e210309923e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training on {torch.cuda.get_device_name(torch.cuda.current_device())}...\")\n",
    "\n",
    "model = Sunbird(\n",
    "    src_type_vocab_size=src_type_vocab_size,\n",
    "    src_text_vocab_size=src_text_vocab_size,\n",
    "    trg_type_vocab_size=trg_type_vocab_size,\n",
    "    trg_text_vocab_size=trg_text_vocab_size,\n",
    "    src_pad_idx=src_pad_idx,\n",
    "    max_datapoint_len=max_seq_len,\n",
    ").to(device)\n",
    "\n",
    "training_loop(\n",
    "    model=model,\n",
    "    num_epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    src_pad_idx=src_pad_idx,\n",
    "    device=device,\n",
    "    interface=interface,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
